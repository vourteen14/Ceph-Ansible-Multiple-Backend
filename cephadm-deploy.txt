#####################
# Node Minimal Spec #
#####################
Proc: 1 Core
Mem: 4G
Disk 1(OS): 24G(Min) /dev/sda
Disk 2(hdd): 5G(Min) /dev/sdb
Disk 3(ssd): 5G(Min) /dev/sdc
OS: Ubuntu 20.04

Hostname: ceph01,ceph02,ceph03
IP: 10.10.10.111, 10.10.10.112, 10.10.10.113
User: ubuntu
Pass: ubuntu

I'm using 3 node with same spec and /etc/hosts was modified mapping IP to hostname 

###############
# My Net Conf #
###############
NIC 1: 10.10.10.0/24

####################
# On All Ceph Node #
####################
$ sudo apt install chrony -y

$ nano /etc/chrony/chrony.conf >>
  ...
  pool 0.id.pool.ntp.org iburst
  pool 1.id.pool.ntp.org iburst
  pool 2.id.pool.ntp.org iburst
  pool 3.id.pool.ntp.org iburst

$ sudo systemctl restart chronyd

$ sudo apt install apt-transport-https ca-certificates curl gnupg-agent software-properties-common -y
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
$ echo "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -sc) stable" | sudo tee /etc/apt/sources.list.d/docker-ce.list
$ sudo apt update
$ sudo apt install docker-ce docker-ce-cli containerd.io -y
$ sudo systemctl enable --now docker

###################
# Do on CEPH01!!! #
###################
$ ssh-keygen -t rsa 
$ sudo apt install sshpass -y
$ for i in `seq 1 3`; do sshpass -p ubuntu ssh-copy-id -i /home/ubuntu/.ssh/id_rsa.pub ubuntu@ceph0$i; done

$ curl --silent --remote-name --location https://github.com/ceph/ceph/raw/quincy/src/cephadm/cephadm
$ chmod +x cephadm

$ sudo ./cephadm add-repo --release quincy
$ sudo ./cephadm install
$ sudo ./cephadm bootstrap --mon-ip 10.10.10.111

############################################################################################
## After bootstraping Ceph, it will show auth info for managing Ceph using GUI,          ###
## Directly open the IP address and credentials provided or you will forget the password ###
############################################################################################
$ sudo ceph telemetry enable channel perf
$ sudo ceph telemetry on --license sharing-1-0
$ sudo ceph config set global osd_max_object_size 4G

$ sudo ceph cephadm get-ssh-config > ssh_config
$ sudo ceph config-key get mgr/cephadm/ssh_identity_key > key
$ sudo chmod 600 key
$ ssh-keygen -y -f key > key.pub
$ for i in `seq 1 3`; do scp key.pub ubuntu@ceph0$i:/home/ubuntu; done

##########################
# SSHD Setup On All Node #
##########################
$ sudo nano /etc/ssh/sshd_config >>
  ...
  PermitRootLogin yes
$ sudo systemctl restart ssh sshd
$ cat key.pub | sudo tee -a /root/.ssh/authorized_keys

#############################################
# Add Host to Cluster do below on CEPH01!!! #
#############################################
$ sudo ceph orch host add ceph02 10.10.10.111 --labels _admin
$ sudo ceph orch host add ceph03 10.10.10.112 --labels _admin

############################################
# Add OSD to Cluster do below on CEPH01!!! #
############################################
$ ceph orch daemon add osd ceph01:data_devices=/dev/sdb,/dev/sdc
$ ceph orch daemon add osd ceph02:data_devices=/dev/sdb,/dev/sdc
$ ceph orch daemon add osd ceph03:data_devices=/dev/sdb,/dev/sdc

####################
## CEPH INIT DONE ##
####################

##################
# CEPH GUI SETUP #
##################
1. Monitor Service Setup
   - Go => Cluster => Service => Click mon => Click Edit
   - Edit like => 
2. Create Ruleset for distinguish SATA and SSD
   - Go => Pools => Create => Point to CRUSH => Click + Button => Setup as below
   - For SATA =>  
   - For SSD => 
   - then Click Create Crush Rule
3. Create Pool hdd & ssd
   - Go => Pools > Create => Setup as below
   - For SATA => 
   - For SSD =>
   
#################   
## BACK TO CLI on Ceph01 ##
#################

#######################
# Upload file testing #
#######################
$ fallocate -l 2G file.img
$ sudo rados put file.img file.img --pool=test-sata
$ sudo rados put file.img file.img --pool=test-ssd

#######################
# Check File Location #
#######################
$ sudo rados -p test-sata ls => Success if show file.img
$ sudo ceph osd map test-sata file.img => 
  osdmap e469 pool 'test-sata' (24) object 'file.img' -> pg 24.b0eee320 (24.0) -> up ([11,7,5], p11) acting ([11,7,5], p11)
  NOTE: 11,7,5 is OSDs where file.img replicated

DO SAME ON SSD POOL IF YOU CURIUS

##########################
# DOWNLOAD FILE FROM OSD #
##########################
$ sudo rados get file.img file.img --pool=test-sata
$ ls file.img
